{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nathan/FacialAttributes/.venv/lib/python3.7/site-packages/skimage/io/manage_plugins.py:23: UserWarning: Your installed pillow version is < 7.1.0. Several security issues (CVE-2020-11538, CVE-2020-10379, CVE-2020-10994, CVE-2020-10177) have been fixed in pillow 7.1.0 or higher. We recommend to upgrade this library.\n",
      "  from .collection import imread_collection_wrapper\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "from skimage import io, transform\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 4699), started 0:02:53 ago. (Use '!kill 4699' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-a4ce59b5e521fffb\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-a4ce59b5e521fffb\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Setup tensorboard\n",
    "import tensorboard\n",
    "\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir runs\n",
    "\n",
    "# If you run this notebook locally, you can also access Tensorboard at 127.0.0.1:6006 now.\n",
    "\n",
    "# Clean up old logs\n",
    "if os.path.isdir('./runs/'):\n",
    "    import shutil\n",
    "    shutil.rmtree('runs/')\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# default `log_dir` is \"runs\"\n",
    "writer = SummaryWriter('runs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing.\n",
    "if False:\n",
    "    # Remove all people without pictures.\n",
    "    df_copy = pd.DataFrame().reindex_like(attributes)\n",
    "    print(len(df_copy))\n",
    "    df_copy = df_copy.iloc[0:0]\n",
    "    print(len(df_copy))\n",
    "#    attributes = pd.read_csv(\"person.csv\", sep=';')\n",
    "#    original_attributes = attributes.copy()\n",
    "\n",
    "#    df_copy = pd.DataFrame().reindex_like(attributes)\n",
    "#    df_copy = df_copy.iloc[0:0]\n",
    "#    counter_no_img = 0\n",
    "\n",
    "#    for index, row in attributes.iterrows():\n",
    "#            id_nr = row['id'] + '.jpg'\n",
    "#            img_path = 'front/front/'\n",
    "#            img = mpimg.imread(img_path + id_nr)\n",
    "\n",
    "            # histogram, bin_edges = np.histogram(img[:, :, 0], bins=256, range=(0, 256))\n",
    "            # occurrences = np.count_nonzero(histogram == 0)\n",
    "            # print(occurrences)\n",
    "#            if index % 2500 == 0:\n",
    "#                print(index, counter_no_img)\n",
    "#            if img.shape[2] == 4: # occurrences>200:\n",
    "#                counter_no_img += 1\n",
    "#                df_copy = df_copy.append(original_attributes.iloc[index])\n",
    "#                attributes = attributes.drop(original_attributes.index[index])\n",
    "\n",
    "#    print(len(attributes))\n",
    "\n",
    "#    df_copy.to_csv(\"attributesPersonsNoImages.csv\", index=False)\n",
    "#    attributes.to_csv(\"attributesPersonsWithImages.csv\", index=False)\n",
    "\n",
    "    # Use this when the people without images are already filtered out.\n",
    "    attributes = pd.read_csv(\"attributesPersonsWithImages.csv\")\n",
    "    \n",
    "    test_frac = 0.15\n",
    "    frac = 1 - test_frac\n",
    "    val_frac = 0.05 / frac # make percentage relative to train_set\n",
    "    \n",
    "    # Split test off the training set.\n",
    "    temp_train_set = attributes.sample(frac=0.85, random_state=3072021)\n",
    "    test_set = attributes.drop(temp_train_set.index)\n",
    "    \n",
    "    # Split validation off the training set.\n",
    "    train_set = temp_train_set.sample(frac=1 - val_frac, random_state=14072021)\n",
    "    val_set = temp_train_set.drop(train_set.index)\n",
    "\n",
    "    train_set.to_csv(\"trainSet.csv\", index=False)\n",
    "    val_set.to_csv('valSet.csv', index=False)\n",
    "    test_set.to_csv(\"testSet.csv\", index=False)\n",
    "\n",
    "#persons_with_no_images = pd.read_csv(\"attributesPersonsNoImages.csv\")\n",
    "#persons_attributes = pd.read_csv(\"attributesPersonsWithImages.csv\")\n",
    "\n",
    "#train_set = pd.read_csv(\"trainSet.csv\")\n",
    "#test_set = pd.read_csv(\"testSet.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceDataset(Dataset):\n",
    "    \"\"\"Face dataset.\"\"\"\n",
    "    \n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied.\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.attributes = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.attributes)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        img_name = os.path.join(self.root_dir,\n",
    "                                self.attributes.loc[idx, 'id'] + '.jpg')\n",
    "        image = Image.open(img_name)\n",
    "        \n",
    "        # Create a dictionary with the properties of the image\n",
    "        # and the image itself.\n",
    "        sample = self.attributes.iloc[0, :].to_dict()\n",
    "        sample['image'] = image\n",
    "        \n",
    "        if self.transform:\n",
    "            sample['image'] = self.transform(sample['image'])\n",
    "        \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define more abstract U-Net classes\n",
    "# Source: https://github.com/milesial/Pytorch-UNet/blob/6aa14cbbc445672d97190fec06d5568a0a004740/unet/unet_parts.py#L28\n",
    "\n",
    "# Constructed according to typical behaviour in the U-Net\n",
    "# where two convolutions with kernel=3 are stacked.\n",
    "class DoubleConv(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
    "        super().__init__()\n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels\n",
    "            \n",
    "        # In U-net we do two convolutions, but keep the channels the same\n",
    "        # (so out_channels twice).\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "# Constructed according to typical behaviour in the U-Net\n",
    "# where we downsample, immediately followed by doubling the\n",
    "# channels by 2.\n",
    "class Down(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "# Does the reverse of the Down class, since we want\n",
    "# to reconstruct the image.\n",
    "class Up(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, bilinear):\n",
    "        super().__init__()\n",
    "        \n",
    "        # If bilinear, use the normal convolution to reduce the number of channels.\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "            # Note that here we don't have the halve the in_channels to account for the concatenation.\n",
    "            # This is because this is already compensated for in the UNet itself. You could see it as\n",
    "            # the previous layer taking this already into account and therefore outputting half the\n",
    "            # channel size for the upsampling method.\n",
    "            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
    "            \n",
    "            # Note that switching to in_channels again (as opposed to in_channels // 2),\n",
    "            # makes sense because this includes the added features from the encoding part,\n",
    "            # which doubles the in_channels size.\n",
    "            self.conv = DoubleConv(in_channels, out_channels)\n",
    "        \n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "\n",
    "        # Input is BCHW (batch, channel, height, width)\n",
    "        # In order to account for the correct height and width size\n",
    "        # when concatenating, we have to pad x1 to match x2.\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "\n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "                        diffY // 2, diffY - diffY // 2])\n",
    "\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a convolutional neural network\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, bilinear):\n",
    "        super(UNet, self).__init__()\n",
    "        \n",
    "        self.n_channels = 3\n",
    "        self.bilinear = bilinear\n",
    "        \n",
    "        factor = 2 if bilinear else 1\n",
    "\n",
    "        self.inc = DoubleConv(self.n_channels, 64)\n",
    "        self.down1 = Down(64, 128)\n",
    "        self.down2 = Down(128, 256)\n",
    "        self.down3 = Down(256, 512)\n",
    "        self.down4 = Down(512, 1024 // factor)\n",
    "        self.up1 = Up(1024, 512 // factor, bilinear)\n",
    "        self.up2 = Up(512, 256 // factor, bilinear)\n",
    "        self.up3 = Up(256, 128 // factor, bilinear)\n",
    "        self.up4 = Up(128, 64, bilinear)\n",
    "        self.outc = nn.Conv2d(64, self.n_channels, kernel_size=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        x = self.outc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data.\n",
    "train_set = FaceDataset(csv_file=\"trainSet.csv\",\n",
    "                        root_dir='data/front/front',\n",
    "                        transform=transforms.Compose([\n",
    "                            transforms.Resize((48, 48)),\n",
    "                            transforms.ToTensor()\n",
    "                        ]))\n",
    "\n",
    "test_set = FaceDataset(csv_file=\"testSet.csv\",\n",
    "                       root_dir='data/front/front',\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.Resize((48, 48)),\n",
    "                           transforms.ToTensor()\n",
    "                       ]))\n",
    "\n",
    "trainloader = DataLoader(train_set, batch_size=4, shuffle=True, num_workers=0)\n",
    "\n",
    "# Show the data in Tensorboard\n",
    "dataiter = iter(trainloader)\n",
    "data = dataiter.next()\n",
    "img_grid = utils.make_grid(data['image'])\n",
    "writer.add_image('dataset_images', img_grid)\n",
    "\n",
    "# Keeps track of how often we train the model,\n",
    "# this way we will see the loss logs in different plots.\n",
    "n_runs = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nathan/FacialAttributes/.venv/lib/python3.7/site-packages/torch/nn/functional.py:3458: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training\n"
     ]
    }
   ],
   "source": [
    "net = UNet(bilinear=True)\n",
    "\n",
    "criterion = nn.MSELoss() # average loss over the whole reconstructed image.\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "# Add a scheme of our network to Tensorboard.\n",
    "writer.add_graph(net.cpu(), data['image'])\n",
    "writer.close()\n",
    "\n",
    "writer = SummaryWriter('runs/{}'.format(n_runs))\n",
    "\n",
    "# Train the network\n",
    "for epoch in range(1):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, start=0):\n",
    "        inputs = data['image']\n",
    "        labels = data['image']\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Log the loss to Tensorboard,\n",
    "        # giving a nice loss over time.\n",
    "        writer.add_scalar('Loss/train', \n",
    "                          loss.item(), \n",
    "                          epoch * len(trainloader) + i)\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            # Get images and swap axes.\n",
    "            image_output = outputs[0].detach().numpy().T\n",
    "            image_output = np.swapaxes(image_output, 0, 1)\n",
    "            image_input = labels[0].detach().numpy().T\n",
    "            image_input = np.swapaxes(image_input, 0, 1)\n",
    "\n",
    "            # Add a comparison between input and output image to Tensorboard.\n",
    "            fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "            ax1.imshow(image_input)\n",
    "            ax2.imshow((image_output * 255).astype(np.uint8))\n",
    "\n",
    "            writer.add_figure('input vs. output',\n",
    "                              fig,\n",
    "                              global_step=epoch * len(trainloader) + i)\n",
    "\n",
    "        if i == 100:\n",
    "            break\n",
    "\n",
    "n_runs = n_runs + 1\n",
    "print('Finished training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Show the feature maps for the latent space.\n",
    "# Probably we want to either change either\n",
    "# the latent space itself or the visualisation.\n",
    "\n",
    "# Calling 'data' on weights makes\n",
    "# a copy of the tensor for local use.\n",
    "latent_conv_weights = net.down4.maxpool_conv[1].double_conv[3].weight.data\n",
    "\n",
    "for i, kernel in enumerate(latent_conv_weights[0]):\n",
    "    fig, ax1 = plt.subplots(1, 1)\n",
    "    ax1.imshow((kernel.numpy()))\n",
    "    writer.add_figure('Latent space weights',\n",
    "                      fig,\n",
    "                      global_step=i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.close()\n",
    "writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
